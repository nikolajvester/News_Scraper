{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea23a5f4",
   "metadata": {},
   "source": [
    "# üì∞ AI-Powered News Aggregator ‚Äì Project Breakdown\n",
    "\n",
    "## üìå Summary\n",
    "This project is a self-contained, AI-driven news aggregator focused on Artificial Intelligence news. It scrapes news articles, converts them to semantic embeddings, summarizes them, and generates audio for passive consumption. Designed as a solo developer learning tool, it emphasizes real-world application of data pipelines, NLP, and AI agent orchestration.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Implementation Steps\n",
    "\n",
    "### üõ†Ô∏è 1. Project Setup\n",
    "- [ ] Set up Python environment (venv or Docker).\n",
    "- [ ] Organize folder structure:\n",
    "    /ingestion\n",
    "    /processing\n",
    "    /storage\n",
    "    /ai\n",
    "    /orchestration\n",
    "    /audio\n",
    "    /interface\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üåê 2. Data Ingestion: Web Scraping\n",
    "- [ ] Use **Playwright** to scrape JavaScript-heavy news sites.\n",
    "- [ ] Use **BeautifulSoup + lxml** for parsing.\n",
    "- [ ] Extract:\n",
    "- Title\n",
    "- Author (optional)\n",
    "- Publication Date\n",
    "- Main content\n",
    "- [ ] Store both `content_raw` and `content_cleaned`.\n",
    "\n",
    "#### ‚úÖ Ethical Practices\n",
    "- [ ] Respect `robots.txt`\n",
    "- [ ] Apply rate limiting\n",
    "- [ ] Avoid copying verbatim copyrighted material\n",
    "- [ ] Add attribution when using summaries\n",
    "\n",
    "---\n",
    "\n",
    "### üíæ 3. Data Storage\n",
    "- [ ] Use **PostgreSQL + pgvector**\n",
    "- [ ] Create tables:\n",
    "\n",
    "#### `articles`\n",
    "- `article_id`\n",
    "- `url`\n",
    "- `title`\n",
    "- `author`\n",
    "- `publication_date`\n",
    "- `content_raw`\n",
    "- `content_cleaned`\n",
    "- `embedding_vector`\n",
    "- `summary_text`\n",
    "- `audio_path`\n",
    "- `created_at`, `updated_at`\n",
    "\n",
    "#### `user_reading_history`\n",
    "- `interaction_id`\n",
    "- `user_id`\n",
    "- `article_id`\n",
    "- `last_accessed_timestamp`\n",
    "- `read_progress_seconds`\n",
    "- `is_read_complete`\n",
    "- `created_at`, `updated_at`\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ñ 4. AI/ML Services\n",
    "\n",
    "#### a. Text Embedding\n",
    "- [ ] Choose model:\n",
    "- `Gemini Embedding` (API-based, 3K-dim vectors)\n",
    "- `SentenceTransformers` (e.g., `all-MiniLM-L6-v2`)\n",
    "- [ ] Generate embeddings from cleaned content\n",
    "\n",
    "#### b. Summarization\n",
    "- [ ] Use **Abstractive summarization**\n",
    "- [ ] Use Hugging Face (`pipeline(\"summarization\")`)\n",
    "- [ ] Prefer models like `T5` or similar\n",
    "- [ ] Generate summaries for unread/missed articles\n",
    "\n",
    "#### c. Text-to-Speech (TTS)\n",
    "- [ ] Choose a model:\n",
    "- Lightweight: `Kokoro`, `Dia`, `Chatterbox`\n",
    "- Realistic: `Orpheus`, `Sesame CSM`\n",
    "- Simple: `eSpeak` (fallback)\n",
    "- [ ] Store audio file path in `audio_path`\n",
    "\n",
    "---\n",
    "\n",
    "### üß† 5. AI Agent Orchestration (LangChain)\n",
    "- [ ] Use **LangChain** for:\n",
    "- Deciding when to summarize\n",
    "- Running summary + TTS in sequence\n",
    "- Integrating with vector DB for search\n",
    "- [ ] Enable dynamic workflows:\n",
    "- Check missed days\n",
    "- Retrieve relevant articles\n",
    "- Summarize and convert to audio\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ 6. Testing\n",
    "- [ ] Unit test:\n",
    "- Scraper\n",
    "- Parser\n",
    "- Embedding generator\n",
    "- Summarizer\n",
    "- TTS generator\n",
    "- [ ] Integration test end-to-end flow\n",
    "\n",
    "---\n",
    "\n",
    "### üéß 7. Optional User Interface\n",
    "- [ ] Build CLI or Web UI for:\n",
    "- Playback\n",
    "- Reading history\n",
    "- Missed article notifications\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Skills & Tools You'll Practice\n",
    "- Web scraping with Playwright & BeautifulSoup\n",
    "- PostgreSQL + pgvector\n",
    "- Embeddings & Semantic Search\n",
    "- Abstractive Summarization with Hugging Face\n",
    "- Text-to-Speech synthesis\n",
    "- AI agent orchestration via LangChain\n",
    "- Ethical scraping & data handling\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Architecture Recommendation\n",
    "Use **Kappa Architecture**:\n",
    "- Unified streaming/batch pipeline\n",
    "- Replayable logs\n",
    "- Simpler setup for solo development\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Suggested File Structure\n",
    "    /ingestion # Playwright scrapers\n",
    "    /processing # Cleaners & parsers\n",
    "    /storage # DB models and logic\n",
    "    /ai/embedding # Embedding generators\n",
    "    /ai/summarization # Hugging Face models\n",
    "    /ai/tts # Audio generation\n",
    "    /orchestration # LangChain logic\n",
    "    /interface # CLI or Web UI\n",
    "    /config # API keys, db config\n",
    "\n",
    "    \n",
    "---\n",
    "\n",
    "## üìÖ Next Steps\n",
    "1. [ ] Set up scraper for 1 AI news site\n",
    "2. [ ] Create database and tables\n",
    "3. [ ] Generate embeddings and summaries\n",
    "4. [ ] Add audio conversion\n",
    "5. [ ] Connect LangChain agent\n",
    "6. [ ] Optional: build interface for consumption\n",
    "\n",
    "---\n",
    "\n",
    "## üìò References\n",
    "Check original documentation for citations and detailed explanations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
